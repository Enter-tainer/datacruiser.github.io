<!DOCTYPE html>




<html class="theme-next muse" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="集成学习,sklearn,boosting,bagging," />





  <link rel="alternate" href="/atom.xml" title="Datacruiser's Blog" type="application/atom+xml" />






<meta name="description" content="DataWhale暑期学习小组-高级算法梳理第八期Task1。">
<meta name="keywords" content="集成学习,sklearn,boosting,bagging">
<meta property="og:type" content="article">
<meta property="og:title" content="随机森林算法梳理">
<meta property="og:url" content="http://datacruiser.io/2019/08/05/随机森林算法梳理/index.html">
<meta property="og:site_name" content="Datacruiser&#39;s Blog">
<meta property="og:description" content="DataWhale暑期学习小组-高级算法梳理第八期Task1。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://machinelearning-1255641038.cos.ap-chengdu.myqcloud.com/Datacruiser_Blog_Sources/%E5%91%A8%E5%BF%97%E5%8D%8E-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%202019-08-06%2010-15-38_AdaBoost%E7%AE%97%E6%B3%95.png">
<meta property="og:image" content="https://machinelearning-1255641038.cos.ap-chengdu.myqcloud.com/Datacruiser_Blog_Sources/%E5%91%A8%E5%BF%97%E5%8D%8E-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%202019-08-06%2017-10-37_Bagging%E7%AE%97%E6%B3%95.png">
<meta property="og:image" content="https://machinelearning-1255641038.cos.ap-chengdu.myqcloud.com/Datacruiser_Blog_Sources/%E5%91%A8%E5%BF%97%E5%8D%8E-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%202019-08-06%2020-38-42_Stacking%E7%AE%97%E6%B3%95.png">
<meta property="og:image" content="https://machinelearning-1255641038.cos.ap-chengdu.myqcloud.com/Datacruiser_Blog_Sources/randomforesttraining.png">
<meta property="og:updated_time" content="2020-01-31T14:37:28.392Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="随机森林算法梳理">
<meta name="twitter:description" content="DataWhale暑期学习小组-高级算法梳理第八期Task1。">
<meta name="twitter:image" content="https://machinelearning-1255641038.cos.ap-chengdu.myqcloud.com/Datacruiser_Blog_Sources/%E5%91%A8%E5%BF%97%E5%8D%8E-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%202019-08-06%2010-15-38_AdaBoost%E7%AE%97%E6%B3%95.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://datacruiser.io/2019/08/05/随机森林算法梳理/"/>





  <title>随机森林算法梳理 | Datacruiser's Blog</title>
  <style>
  .forkme{
    display: none;
  }
  @media (min-width: 768px) {
  .forkme{
    display: inline;
  }
  }
  </style><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <div class="forkme">
  <a href="https://github.com/datacruiser" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>
  <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
  </div>  

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Datacruiser's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">数海拾荒</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://datacruiser.io/2019/08/05/随机森林算法梳理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Datacruiser">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.imgur.com/qsQpBoA.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Datacruiser's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">随机森林算法梳理</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-05T23:07:21+00:00">
                2019-08-05
              </time>
            

            

            
          </span>

          
             <span class="post-updated">
                &nbsp; | &nbsp; 更新于
            <time itemprop="dateUpdated" datetime="2020-01-31T14:37:28+00:00" content="2020-01-31">
                2020-01-31
             </time>
             </span>
         

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/08/05/随机森林算法梳理/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/08/05/随机森林算法梳理/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  6.7k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  25
                </span>
              
            </div>
          

          
              <div class="post-description">
                  DataWhale暑期学习小组-高级算法梳理第八期Task1。
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="集成学习的概念"><a href="#集成学习的概念" class="headerlink" title="集成学习的概念"></a>集成学习的概念</h1><p>集成学习（ensemble learning）指的是，我们采用多个弱学习进行学习，来替代用一个单一的精密的高效能的学习器对数据进行学习，并且通过一定的手段将这些弱学习器的结果进行整合来完成学习任务的方法，有时也被称为多分类系统（multi-classifier system）、基于委员会的学习（committee-based learning）等。</p>
<p>集成学习通过将多个弱学习器进行结合，常可获得比单一学习器显著优越的泛化性能。这对“弱学习器”尤为明显，因此集成学习的很多理论研究都是针对弱学习器进行的。</p>
<h1 id="个体学习器的概念"><a href="#个体学习器的概念" class="headerlink" title="个体学习器的概念"></a>个体学习器的概念</h1><p>个体学习器（individual learner）通常是用一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网络算法等。此时集成中只包含同种类型的个体学习器，例如“决策树集成”中的个体学习器全是决策树，“神经网络集成”中就全是神经网络，这样的集成是“同质”（homogeneous）的，同质集成中的个体学习器也称为“基学习器”（baselearner），相应的学习算法称为“基学习算法”（baselearning algorithm）。有同质就有异质（heterogeneous），若集成包含不同类型的个体学习器，例如同时包含决策树和神经网络，那么这时个体学习器一般不称为基学习器，而称作“组件学习器”（componentleaner）或直接称为个体学习器。</p>
<h1 id="boosting-bagging的概念、异同点"><a href="#boosting-bagging的概念、异同点" class="headerlink" title="boosting bagging的概念、异同点"></a>boosting bagging的概念、异同点</h1><p>根据个体学习器生成方式的不同，目前的集成学习方法大致可以分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting，后者的代表是Bagging和“随机森林”（Random Forest）。</p>
<h2 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h2><p>首先需要明确一个概念，在概率近似正确（PAC）学习框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。并且可以从理论上面证明，强可学习和弱可学习是等价的。那么问题来了，在学习当中，如何将比较容易发现的“弱学习算法”提升（boosting）为“强学习算法”呢？这便是<strong>boosting</strong>需要解决的问题。</p>
<p><strong>boosting</strong>这族算法的工作机制为：先从初识训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本分布训练下一个基学习器；如此重复进行，直到基学习器数据达到事先指定的值$T$，最终将这$T$个基学习器进行加权结合。</p>
<p><strong>Boosting</strong>族算法最著名的代表是AdaBoost，其描述如下图所示，其中$y_i\in${-1,+1}，$f$是真实函数。</p>
<p><img src="https://machinelearning-1255641038.cos.ap-chengdu.myqcloud.com/Datacruiser_Blog_Sources/%E5%91%A8%E5%BF%97%E5%8D%8E-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%202019-08-06%2010-15-38_AdaBoost%E7%AE%97%E6%B3%95.png" alt="AdaBoost算法"></p>
<h2 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h2><p>为了得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立；虽然“独立”在现实任务中无法做到，但可以设法 使基学习器尽可能具有较大的差异。给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器。这样，由于训练数据不同，我们获得的基学习器可望具有比较大的差距。</p>
<p><strong>bagging</strong>正是借鉴这样的思想，直接基于自助采样法（bootstrap sampling），是并行式集成学习方法最著名的代表。给定包含$m$个样本的数据集，我们先随机去除一个样本放入采样集当中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样经过$m$次随机采样操作，我们得到含$m$个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的从未出现。约63.2%的样本出现在采样集中，而未出现的约36.8%的样本可用作验证集来对后续的泛化性能进行“包外估计”。</p>
<p>根据基学习器的不同，包外估计还有其他用途。当基学习器是决策树时，可以使用包外样本来辅助剪枝，或者用于估计决策树中各个结点的后验概率以辅助对零训练样本结点的处理；当基学习器是神经网络时，可使用包外样本来辅助早期停止来减小过拟合风险。</p>
<p>这样类推，我们可以采用出$T$个含$m$个训练样本的采样集，然后基于每个采样集训练出一个基学习器。再将这些基学习器进行结合，针对预测输出任务的不同，分类任务通常使用简单投票法，回归任务通常使用简单平均法，这就是<strong>Bagging</strong>的基本流程。算法描述如下图所示。</p>
<p><img src="https://machinelearning-1255641038.cos.ap-chengdu.myqcloud.com/Datacruiser_Blog_Sources/%E5%91%A8%E5%BF%97%E5%8D%8E-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%202019-08-06%2017-10-37_Bagging%E7%AE%97%E6%B3%95.png" alt="Bagging算法"></p>
<h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><ul>
<li>Bagging的个体学习器不存在强依赖关系，可以同时生成，比较便于并行化计算；Boosting的个体学习器存在强依赖关系，只能串行生成，后续的一些开源算法，比如XGBoost以及LightGBM的并行化都只是建树过程的分布式，学习本身还是串行的；</li>
<li>从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等易受到样本扰动的学习器上效用更明显；Boosting主要关注降低偏差，因此Boosting基于泛化性能相当弱的学习器构建出很强的集成。</li>
<li>说说相同的，这两个算法如果采用相同的基学习器，那么算法的时间复杂度是基本相同的。</li>
</ul>
<h1 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h1><p>学习器结合策略的好处有以下三个方面： </p>
<ul>
<li>从统计的方面来看，结合多个学习器可以减小因学习任务的假设空间太大导致仅仅使用单学习器可能因误选而导致泛化性能不佳的风险；</li>
<li>从计算的方面来看，通过多次运行不同的学习算法之后进行结合可降低陷入糟糕局部极小点的风险；</li>
<li>从表示的方面来看，通过结合多个学习器，扩大了相应的假设空间，有可能学得到更好的近视。<h2 id="平均法"><a href="#平均法" class="headerlink" title="平均法"></a>平均法</h2></li>
</ul>
<p>对于数值型输出$h_i(x)\in R$，最常见的结合策略是使用平均法。</p>
<ul>
<li>简单平均法</li>
</ul>
<p>$$H(x)=\frac{1}{T}\sum_{i=1}^Th_i(x)$$</p>
<ul>
<li>加权平均法</li>
</ul>
<p>$$H(x)=\frac{1}{T}\sum_{i=1}^T\omega_ih_i(x)$$</p>
<p>其中$\omega_i$是个体学习器$h_i$的权重，通常要求$\omega_i\geqslant0,\sum_{i=1}^T\omega_i=1$。</p>
<p>显然，简单平均法是加权平均法的特例，加权平均法的权重一般是从训练数据中学习而得。一般而言，在个体学习器性能相差较大时宜使用加权平均法，而在个体学习器性能相近时使用简单平均法。</p>
<h2 id="投票法"><a href="#投票法" class="headerlink" title="投票法"></a>投票法</h2><p>对于分类任务来说，学习器从类别标记集合中预测出一个标记，最常见的组合策略时使用投票法。</p>
<ul>
<li>绝对多数投票法</li>
</ul>
<p>即若某标记得票过半数，则预测为该标记；否则拒绝预测。</p>
<ul>
<li>相对多数投票法</li>
</ul>
<p>$$H(x)=c_\underset{j}{\mathrm{argmax}}\sum_{i=1}^Th_i^j(x)$$</p>
<p>即预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个。</p>
<ul>
<li>加权投票法</li>
</ul>
<p>$$H(x)=c_\underset{j}{\mathrm{argmax}}\sum_{i=1}^T\omega_ih_i^j(x)$$</p>
<p>与加权平均法类似，其中$\omega_i$是个体学习器$h_i$的权重，通常要求$\omega_i\geqslant0,\sum_{i=1}^T\omega_i=1$。</p>
<p>需要补充的是,在现实任务中，不同类型个体学习器可能产生不同类型的$h_i^j(x)$值，比如：</p>
<ul>
<li>类标记：$h_i^j(x)\in${0,1}，所谓“硬投票”</li>
<li>类概率：$h_i^j(x)\in$[0,1]，所谓“软投票”</li>
</ul>
<p>必须注意，不同类型的$h_i^j(x)$值不能混用，必要的时候需要适当转化。</p>
<h2 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h2><p>在训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另外一个学习器来进行结合，大名鼎鼎的Stacking就是学习法的典型代表。我们不妨把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或者元学习器（meta-learner）。</p>
<p>Stacking先从初始数据集训练出初级学习器，然后“生成”一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。Stacking的算法描述如下所示，我们假定初级学习器使用了不同的学习算法产生。</p>
<p><img src="https://machinelearning-1255641038.cos.ap-chengdu.myqcloud.com/Datacruiser_Blog_Sources/%E5%91%A8%E5%BF%97%E5%8D%8E-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%202019-08-06%2020-38-42_Stacking%E7%AE%97%E6%B3%95.png" alt="Stacking"></p>
<h1 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h1><p>随机森林是Bagging的一个扩展变体，在理解了Bagging方法后，随机森林学习起来就容易多了。RF在以决策树作为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中加入了随机属性的选择。</p>
<p>具体来说，传统决策树在选择划分属性时是在当前结点的所有候选属性（假定有$d$个）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的候选属性集合中随机选择一个包含$k$个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数$k$控制了随机性的引入程度：若令$k=d$，则基决策树的构建与传统决策树相同；若令$k=1$，则是随机选择一个属性用于划分；显然，抽取的属性数$k$的选择比较重要，一般推荐 $k=log_2d$ 。由此，随机森林的基学习器的“多样性”不仅来自样本的扰动，还来自属性的扰动，使得最终集成的泛化能力进一步提升。</p>
<h2 id="随机森林的思想"><a href="#随机森林的思想" class="headerlink" title="随机森林的思想"></a>随机森林的思想</h2><p>除了作为一个bagging类的集成算法本身的构思，比如通过弱学习器的集成来提供精度和泛化能力，随机森林本身的主要思想在于随机性，通过随机性，提高模型的泛化能力，主要体现在以下三个方面：</p>
<ul>
<li>对训练样本的随机采样</li>
<li>对属性的随机采样</li>
<li>基于随机采样的属性的决策树构造</li>
</ul>
<p><img src="https://machinelearning-1255641038.cos.ap-chengdu.myqcloud.com/Datacruiser_Blog_Sources/randomforesttraining.png" alt="随机森林"></p>
<h2 id="随机森林的推广"><a href="#随机森林的推广" class="headerlink" title="随机森林的推广"></a>随机森林的推广</h2><p>由于RF在实际应用中的良好特性，基于RF，有很多变种算法，应用也很广泛，不光可以用于分类回归，还可以用于特征转换，异常点检测等。下面对于这些RF家族的算法中有代表性的做一个总结。</p>
<ul>
<li>extra trees</li>
</ul>
<p>extra trees是RF的一个变种, 原理几乎和RF一模一样，仅有区别有：</p>
<p>对于每个决策树的训练集，RF采用的是随机采样bootstrap来选择采样集作为每个决策树的训练集，而extra trees一般不采用随机采样，即每个决策树采用原始训练集。</p>
<p>在选定了划分特征后，RF的决策树会基于基尼系数，均方差之类的原则，选择一个最优的特征值划分点，这和传统的决策树相同。但是extra trees比较的激进，他会随机的选择一个特征值来划分决策树。</p>
<p>从第二点可以看出，由于随机选择了特征值的划分点位，而不是最优点位，这样会导致生成的决策树的规模一般会大于RF所生成的决策树。也就是说，模型的方差相对于RF进一步减少，但是偏倚相对于RF进一步增大。在某些时候，extra trees的泛化能力比RF更好。</p>
<ul>
<li>Totally Random Trees Embedding</li>
</ul>
<p>Totally Random Trees Embedding(以下简称 TRTE)是一种非监督学习的数据转化方法。它将低维的数据集映射到高维，从而让映射到高维的数据更好的运用于分类回归模型。我们知道，在支持向量机中运用了核方法来将低维的数据集映射到高维，此处TRTE提供了另外一种方法。</p>
<p>TRTE在数据转化的过程也使用了类似于RF的方法，建立T个决策树来拟合数据。当决策树建立完毕以后，数据集里的每个数据在T个决策树中叶子节点的位置也定下来了。比如我们有3颗决策树，每个决策树有5个叶子节点，某个数据特征x划分到第一个决策树的第2个叶子节点，第二个决策树的第3个叶子节点，第三个决策树的第5个叶子节点。则x映射后的特征编码为(0,1,0,0,0,     0,0,1,0,0,     0,0,0,0,1), 有15维的高维特征。这里特征维度之间加上空格是为了强调三颗决策树各自的子编码。</p>
<p>映射到高维特征后，可以继续使用监督学习的各种分类回归算法了。</p>
<ul>
<li>Isolation Forest</li>
</ul>
<p>Isolation Forest（以下简称IForest）是一种异常点检测的方法。它也使用了类似于RF的方法来检测异常点。</p>
<p>对于在T个决策树的样本集，IForest也会对训练集进行随机采样,但是采样个数不需要和RF一样，对于RF，需要采样到采样集样本个数等于训练集个数。但是IForest不需要采样这么多，一般来说，采样个数要远远小于训练集个数？为什么呢？因为我们的目的是异常点检测，只需要部分的样本我们一般就可以将异常点区别出来了。</p>
<p>对于每一个决策树的建立， IForest采用随机选择一个划分特征，对划分特征随机选择一个划分阈值。这点也和RF不同。</p>
<p>另外，IForest一般会选择一个比较小的最大决策树深度max_depth，原因同样本采集，用少量的异常点检测一般不需要这么大规模的决策树。</p>
<p>对于异常点的判断，则是将测试样本点x拟合到T颗决策树。计算在每颗决策树上该样本的叶子节点的深度$h_t(x)$。从而可以计算出平均高度$h(x)$。此时我们用下面的公式计算样本点x的异常概率:</p>
<p>$$s(x,m)=2^{-\frac{h(x)}{c(m)}}$$</p>
<p>其中，$m$为样本个数。$c(m)$的表达式为：</p>
<p>$$c(m)=2ln(m-1)+\xi-2\frac{m-1}{m}$$</p>
<p>其中，$\xi$为欧拉常数，$s(x,m)$的取值范围是[0,1]，取值越接近于1，则是异常点的概率也越大。</p>
<h2 id="随机森林的优缺点"><a href="#随机森林的优缺点" class="headerlink" title="随机森林的优缺点"></a>随机森林的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>RF能够解决分类与回归两种类型的问题，并且在这两方面都有较好的表现</li>
<li>RF实现简单，并且是bagging族算法，可以高度并行化，训练速度快，在大数据时代的大样本训练有天然优势</li>
<li>子数据集的构造采用有放回式抽样，使得抽样数据集有大约1/3的数据没有被抽中去参加决策树的构造，可以做“包外估计”，减小过拟合风险</li>
<li>随机选择基学习器决策树节点划分特征，这样便于避免“维度灾难”，在高维时，仍然能高效的训练模型</li>
<li>对部分特征缺失不敏感</li>
<li>在训练后，可以给出各个特征对于输出的重要性</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>虽然对于缺失值不敏感，但是对于噪音比较敏感，随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合</li>
<li>对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的</li>
</ul>
<h2 id="随机森林在sklearn中的参数解释"><a href="#随机森林在sklearn中的参数解释" class="headerlink" title="随机森林在sklearn中的参数解释"></a>随机森林在sklearn中的参数解释</h2><p>在sklearn当中按学习任务的不同一共有两个与随机森林相关的类：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" target="_blank" rel="noopener">RandomForestClassifier</a>应用于分类学习任务，<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor" target="_blank" rel="noopener">RandomForestRegressor</a>应用于回归学习任务。</p>
<h3 id="RandomForestClassifier"><a href="#RandomForestClassifier" class="headerlink" title="RandomForestClassifier"></a>RandomForestClassifier</h3><h4 id="参数（parameters）"><a href="#参数（parameters）" class="headerlink" title="参数（parameters）"></a>参数（parameters）</h4><ul>
<li><p><strong>n_estimators : integer, optional (default=10)</strong></p>
<ul>
<li>随机森林中树的数量，整型，可选项，默认是10</li>
</ul>
</li>
<li><p><strong>criterion : string, optional (default=”gini”)</strong></p>
<ul>
<li>衡量特征分裂质量的函数，这是一个与基学习器决策树相关的参数，通常对于Gini不纯度采用“gini”，对于信息增益用“entropy”。字符串类型，可选项，默认是”gini”</li>
</ul>
</li>
<li><p><strong>max_depth : integer or None, optional (default=None)</strong></p>
<ul>
<li>树的最大深度。如果是None，那么结点会延伸直到所有的叶子都只含相同的样本或者所有的叶子结点包含比<code>min_samples_splits</code>少的样本。整型或者None，可选项，默认是None</li>
</ul>
</li>
<li><p><strong>min_samples_split : int, float, optional (default=2)</strong></p>
<ul>
<li>用于分裂一个内部结点的最小所需的样本数，整型或者浮点型，可选项，默认是2</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>min_samples_leaf : int, float, optional (default=1)</strong></p>
<ul>
<li>一个叶子结点所要求的最小样本个数，这个参数有平滑模型的作用，特别是回归的学习任务里面。整型或者浮点型，可选项，默认是1</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>min_weight_fraction_leaf : float, optional (default=0.)</strong></p>
<ul>
<li>一个叶子结点所有输入样本权重总和的最小权重分数，浮点型，可选项，默认为0.</li>
</ul>
</li>
<li><p><strong>max_features : int, float, string or None, optional (default=”auto”)</strong></p>
<ul>
<li><p>寻找最优分裂时所考虑的特征个数，整型、浮点型、字符串或者None，可选项，默认是“auto”</p>
<ul>
<li>int, then consider max_features features at each split.</li>
<li>float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.</li>
<li>“auto”, then max_features=sqrt(n_features).</li>
<li>“sqrt”, then max_features=sqrt(n_features) (same as “auto”).</li>
<li>“log2”, then max_features=log2(n_features).</li>
<li>None, then max_features=n_features.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>max_leaf_nodes : int or None, optional (default=None)</strong></p>
<ul>
<li>以最好的方式用<code>max_leaf_nodes</code>来建树，如果是Nonde的话则对叶子结点的个数不限制，整型或者Node，可选项，默认是Node</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>min_impurity_decrease : float, optional (default=0.)</strong></p>
<ul>
<li>最小不纯度减少值，结点将会继续分裂，如果这个分裂对于不纯度的减少能够大于或者等于该值。浮点型，可选项，默认为0.</li>
</ul>
</li>
<li><p><strong>min_impurity_split : float, (default=1e-7)</strong></p>
<ul>
<li>建树过程中早起停止的阈值。如果节点的不纯度大于该值将会继续分裂，否则停止，成为叶子结点。浮点型，默认为1e-7</li>
<li>后续将会被<code>min_impurity_decrease</code>参数替代</li>
</ul>
</li>
<li><p><strong>bootstrap : boolean, optional (default=True)</strong></p>
<ul>
<li>建树时将采用有放回自采样（bootstrap sample），如果是False，那么将用全量数据建树。布尔型，可选项，默认为True</li>
</ul>
</li>
<li><p><strong>oob_score : bool (default=False)</strong></p>
<ul>
<li>是否采用包外样本来估计泛化准确性，布尔型，默认为False</li>
</ul>
</li>
<li><p><strong>n_jobs : int or None, optional (default=None)</strong></p>
<ul>
<li>在学习和预测过程当中并行计算的任务数量。None意味着1，-1意味着使用所有的处理器。整型或者None，可选项，默认为None</li>
</ul>
</li>
<li><p><strong>random_state : int, RandomState instance or None, optional (default=None)</strong></p>
<ul>
<li>随机状态，有以下三种方式<ul>
<li>int, random_state is the seed used by the random number generator; </li>
<li>RandomState instance, random_state is the random number generator; </li>
<li>None, the random number generator is the RandomState instance used by np.random.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>verbose : int, optional (default=0)</strong></p>
<ul>
<li>控制在学习和预测时是否输出中间结果，整型，可选项，默认是0</li>
</ul>
</li>
<li><p><strong>warm_start : bool, optional (default=False)</strong></p>
<ul>
<li>布尔型，可选项，默认为False，如果设置为True，将重复使用上一次调用的解来学习并加入更多的学习器到集成算法当中</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>class_weight : dict, list of dicts, “balanced”, “balanced_subsample” or None, optional (default=None)</strong></p>
<ul>
<li>类别权重，字典，或者字典列表，可选项，默认为None</li>
</ul>
</li>
</ul>
<h4 id="属性（attributes）"><a href="#属性（attributes）" class="headerlink" title="属性（attributes）"></a>属性（attributes）</h4><ul>
<li><p><strong>estimators_ : list of DecisionTreeClassifier</strong></p>
<ul>
<li>拟合后的基学习器集合</li>
</ul>
</li>
<li><p><strong>classes_ : array of shape = [n_classes] or a list of such arrays</strong></p>
<ul>
<li>类别标签（二分类），或者类别标签数组列表（多分类）</li>
</ul>
</li>
<li><p><strong>n_classes_ : int or list</strong></p>
<ul>
<li>类别数量或者对于每一个输出类别的数量列表</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>n_features_ : int</strong></p>
<ul>
<li>特征数量</li>
</ul>
</li>
<li><p><strong>n_outputs_ : int</strong></p>
<ul>
<li>输出数量</li>
</ul>
</li>
<li><p><strong>feature_importances_ : array of shape = [n_features]</strong></p>
<ul>
<li>特征重要性系数</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>oob_score_ : float</strong></p>
<ul>
<li>包外样本估计效果，浮点型</li>
</ul>
</li>
<li><p><strong>oob_decision_function_ : array of shape = [n_samples, n_classes]</strong></p>
<ul>
<li>包外样本预测结果</li>
</ul>
</li>
</ul>
<h4 id="方法（methods）"><a href="#方法（methods）" class="headerlink" title="方法（methods）"></a>方法（methods）</h4><table>
<thead>
<tr>
<th style="text-align:center">方法名称</th>
<th style="text-align:center">方法解释 </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">apply(self, X)</td>
<td style="text-align:center">Apply trees in the forest to X, return leaf indices.</td>
</tr>
<tr>
<td style="text-align:center">decision_path(self, X)</td>
<td style="text-align:center">Return the decision path in the forest</td>
</tr>
<tr>
<td style="text-align:center">fit(self, X, y[, sample_weight])</td>
<td style="text-align:center">Build a forest of trees from the training set (X, y).</td>
</tr>
<tr>
<td style="text-align:center">get_params(self[, deep])</td>
<td style="text-align:center">Get parameters for this estimator.</td>
</tr>
<tr>
<td style="text-align:center">predict(self, X)</td>
<td style="text-align:center">Predict class for X.</td>
</tr>
<tr>
<td style="text-align:center">predict_log_proba(self, X)</td>
<td style="text-align:center">Predict class log-probabilities for X.</td>
</tr>
<tr>
<td style="text-align:center">predict_proba(self, X)</td>
<td style="text-align:center">Predict class probabilities for X.</td>
</tr>
<tr>
<td style="text-align:center">score(self, X, y[, sample_weight])</td>
<td style="text-align:center">Returns the mean accuracy on the given test data and labels.</td>
</tr>
<tr>
<td style="text-align:center">set_params(self, **params)</td>
<td style="text-align:center">Set the parameters of this estimator.</td>
</tr>
</tbody>
</table>
<h3 id="RandomForestRegressor"><a href="#RandomForestRegressor" class="headerlink" title="RandomForestRegressor"></a>RandomForestRegressor</h3><h4 id="参数（parameters）-1"><a href="#参数（parameters）-1" class="headerlink" title="参数（parameters）"></a>参数（parameters）</h4><ul>
<li><p><strong>n_estimators : integer, optional (default=10)</strong></p>
<ul>
<li>随机森林中树的数量，整型，可选项，默认是10</li>
</ul>
</li>
<li><p><strong>criterion : string, optional (default=”mse”)</strong></p>
<ul>
<li>衡量特征分裂质量的函数，字符串类型，可选项，默认是”mse”</li>
</ul>
</li>
<li><p><strong>max_depth : integer or None, optional (default=None)</strong></p>
<ul>
<li>树的最大深度。如果是None，那么结点会延伸直到所有的叶子都只含相同的样本或者所有的叶子结点包含比<code>min_samples_splits</code>少的样本。整型或者None，可选项，默认是None</li>
</ul>
</li>
<li><p><strong>min_samples_split : int, float, optional (default=2)</strong></p>
<ul>
<li>用于分裂一个内部结点的最小所需的样本数，整型或者浮点型，可选项，默认是2</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>min_samples_leaf : int, float, optional (default=1)</strong></p>
<ul>
<li>一个叶子结点所要求的最小样本个数，这个参数有平滑模型的作用，特别是回归的学习任务里面。整型或者浮点型，可选项，默认是1</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>min_weight_fraction_leaf : float, optional (default=0.)</strong></p>
<ul>
<li>一个叶子结点所有输入样本权重总和的最小权重分数，浮点型，可选项，默认为0.</li>
</ul>
</li>
<li><p><strong>max_features : int, float, string or None, optional (default=”auto”)</strong></p>
<ul>
<li><p>寻找最优分裂时所考虑的特征个数，整型、浮点型、字符串或者None，可选项，默认是“auto”</p>
<ul>
<li>int, then consider max_features features at each split.</li>
<li>float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.</li>
<li>“auto”, then max_features=sqrt(n_features).</li>
<li>“sqrt”, then max_features=sqrt(n_features) (same as “auto”).</li>
<li>“log2”, then max_features=log2(n_features).</li>
<li>None, then max_features=n_features.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>max_leaf_nodes : int or None, optional (default=None)</strong></p>
<ul>
<li>以最好的方式用<code>max_leaf_nodes</code>来建树，如果是Nonde的话则对叶子结点的个数不限制，整型或者Node，可选项，默认是Node</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>min_impurity_decrease : float, optional (default=0.)</strong></p>
<ul>
<li>最小不纯度减少值，结点将会继续分裂，如果这个分裂对于不纯度的减少能够大于或者等于该值。浮点型，可选项，默认为0.</li>
</ul>
</li>
<li><p><strong>min_impurity_split : float, (default=1e-7)</strong></p>
</li>
</ul>
<pre><code>- 建树过程中早起停止的阈值。如果节点的不纯度大于该值将会继续分裂，否则停止，成为叶子结点。浮点型，默认为1e-7
- 后续将会被`min_impurity_decrease`参数替代
</code></pre><ul>
<li><strong>bootstrap : boolean, optional (default=True)</strong></li>
</ul>
<pre><code>- 建树时将采用有放回自采样（bootstrap sample），如果是False，那么将用全量数据建树。布尔型，可选项，默认为True
</code></pre><ul>
<li><p><strong>oob_score : bool (default=False)</strong></p>
<ul>
<li>是否采用包外样本来估计为建模数据的$R^2$，布尔型，默认为False</li>
</ul>
</li>
<li><p><strong>n_jobs : int or None, optional (default=None)</strong></p>
<ul>
<li>在学习和预测过程当中并行计算的任务数量。None意味着1，-1意味着使用所有的处理器。整型或者None，可选项，默认为None</li>
</ul>
</li>
<li><p><strong>random_state : int, RandomState instance or None, optional (default=None)</strong></p>
<ul>
<li>随机状态，有以下三种方式<ul>
<li>int, random_state is the seed used by the random number generator; </li>
<li>RandomState instance, random_state is the random number generator; </li>
<li>None, the random number generator is the RandomState instance used by np.random.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>verbose : int, optional (default=0)</strong></p>
<ul>
<li>控制在学习和预测时是否输出中间结果，整型，可选项，默认是0</li>
</ul>
</li>
<li><p><strong>warm_start : bool, optional (default=False)</strong></p>
<ul>
<li>布尔型，可选项，默认为False，如果设置为True，将重复使用上一次调用的解来学习并加入更多的学习器到集成算法当中</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>class_weight : dict, list of dicts, “balanced”, “balanced_subsample” or None, optional (default=None)</strong></p>
<ul>
<li>类别权重，字典，或者字典列表，可选项，默认为None</li>
</ul>
</li>
</ul>
<h4 id="属性（attributes）-1"><a href="#属性（attributes）-1" class="headerlink" title="属性（attributes）"></a>属性（attributes）</h4><ul>
<li><p><strong>estimators_ : list of DecisionTreeClassifier</strong></p>
<ul>
<li>拟合后的基学习器集合</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>n_features_ : int</strong></p>
<ul>
<li>特征数量</li>
</ul>
</li>
<li><p><strong>n_outputs_ : int</strong></p>
<ul>
<li>输出数量</li>
</ul>
</li>
<li><p><strong>feature_importances_ : array of shape = [n_features]</strong></p>
<ul>
<li>特征重要性系数</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>oob_score_ : float</strong></p>
<ul>
<li>包外样本估计效果，浮点型</li>
</ul>
</li>
<li><p><strong>oob_prediction_ : array of shape = [n_samples, n_classes]</strong></p>
<ul>
<li>包外估计在训练集上的预测结果</li>
</ul>
</li>
</ul>
<h4 id="方法（methods）-1"><a href="#方法（methods）-1" class="headerlink" title="方法（methods）"></a>方法（methods）</h4><table>
<thead>
<tr>
<th style="text-align:center">方法名称</th>
<th style="text-align:center">方法解释 </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">apply(self, X)</td>
<td style="text-align:center">Apply trees in the forest to X, return leaf indices.</td>
</tr>
<tr>
<td style="text-align:center">decision_path(self, X)</td>
<td style="text-align:center">Return the decision path in the forest</td>
</tr>
<tr>
<td style="text-align:center">fit(self, X, y[, sample_weight])</td>
<td style="text-align:center">Build a forest of trees from the training set (X, y).</td>
</tr>
<tr>
<td style="text-align:center">get_params(self[, deep])</td>
<td style="text-align:center">Get parameters for this estimator.</td>
</tr>
<tr>
<td style="text-align:center">predict(self, X)</td>
<td style="text-align:center">Predict class for X.</td>
</tr>
<tr>
<td style="text-align:center">score(self, X, y[, sample_weight])</td>
<td style="text-align:center">Returns the mean accuracy on the given test data and labels.</td>
</tr>
<tr>
<td style="text-align:center">set_params(self, **params)</td>
<td style="text-align:center">Set the parameters of this estimator. </td>
</tr>
</tbody>
</table>
<h2 id="随机森林的应用场景"><a href="#随机森林的应用场景" class="headerlink" title="随机森林的应用场景"></a>随机森林的应用场景</h2><p>因为随机森林可以学习分类和回归两种类型的任务，因此有着比较多的应用前景。结合自己的实践主要在以下两个方面使用了随机森林：</p>
<ul>
<li>样本维度不高，对精度要求高的分类任务，具体可以参考下面文章</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/27295874" target="_blank" rel="noopener">随机森林——二元分类的利器之Kaggle初体验Titanic: Machine Learning from Disaster</a></p>
<ul>
<li>因为随机森林可以给出各个特征的重要性，也不失作为一个特征筛选的中间算法过程，具体可以参考下面文章，在信用评分卡的构建当中就用到了随机森林来筛选特征</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/52486437" target="_blank" rel="noopener">随机森林在评分卡构建上的应用</a></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><p><a href="https://book.douban.com/subject/26708119/" target="_blank" rel="noopener">《机器学习》，周志华</a></p>
</li>
<li><p><a href="https://book.douban.com/subject/33437381/" target="_blank" rel="noopener">《统计学习方法》（第二版），李航</a></p>
</li>
<li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" target="_blank" rel="noopener">3.2.4.3.1. sklearn.ensemble.RandomForestClassifier</a></p>
</li>
<li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor" target="_blank" rel="noopener">3.2.4.3.2. sklearn.ensemble.RandomForestRegressor</a></p>
</li>
<li><p><a href="https://scikit-learn.org/stable/modules/ensemble.html#ensemble-methods" target="_blank" rel="noopener">1.11. Ensemble methods</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/pinard/p/6156009.html" target="_blank" rel="noopener">Bagging与随机森林算法原理小结</a></p>
</li>
<li><a href="https://community.alteryx.com/t5/Alteryx-Designer-Knowledge-Base/Seeing-the-Forest-for-the-Trees-An-Introduction-to-Random-Forest/ta-p/158062" target="_blank" rel="noopener">Seeing the Forest for the Trees: An Introduction to Random Forest</a></li>
</ul>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Datacruiser
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://datacruiser.io/2019/08/05/随机森林算法梳理/" title="随机森林算法梳理">http://datacruiser.io/2019/08/05/随机森林算法梳理/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/集成学习/" rel="tag"># 集成学习</a>
          
            <a href="/tags/sklearn/" rel="tag"># sklearn</a>
          
            <a href="/tags/boosting/" rel="tag"># boosting</a>
          
            <a href="/tags/bagging/" rel="tag"># bagging</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/08/05/Leetcode-21-合并两个有序链表/" rel="next" title="Leetcode 21. 合并两个有序链表">
                <i class="fa fa-chevron-left"></i> Leetcode 21. 合并两个有序链表
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/08/06/Leetcode-腾讯精选50题-No-5-122-买卖股票的最佳时机-II/" rel="prev" title="Leetcode 腾讯精选50题 No.5 122. 买卖股票的最佳时机 II">
                Leetcode 腾讯精选50题 No.5 122. 买卖股票的最佳时机 II <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://i.imgur.com/qsQpBoA.jpg"
                alt="Datacruiser" />
            
              <p class="site-author-name" itemprop="name">Datacruiser</p>
              <p class="site-description motion-element" itemprop="description">Achieving what I want in light of what is true.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">159</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">121</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/datacruiser" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/realphy" target="_blank" title="Zhihu">
                    
                      <i class="fa fa-fw fa-group"></i>Zhihu</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://weibo.com/jijiwhywhy" target="_blank" title="Weibo">
                    
                      <i class="fa fa-fw fa-weibo"></i>Weibo</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:phy.zju@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#集成学习的概念"><span class="nav-number">1.</span> <span class="nav-text"><a href="#&#x96C6;&#x6210;&#x5B66;&#x4E60;&#x7684;&#x6982;&#x5FF5;" class="headerlink" title="&#x96C6;&#x6210;&#x5B66;&#x4E60;&#x7684;&#x6982;&#x5FF5;"></a>&#x96C6;&#x6210;&#x5B66;&#x4E60;&#x7684;&#x6982;&#x5FF5;</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#个体学习器的概念"><span class="nav-number">2.</span> <span class="nav-text"><a href="#&#x4E2A;&#x4F53;&#x5B66;&#x4E60;&#x5668;&#x7684;&#x6982;&#x5FF5;" class="headerlink" title="&#x4E2A;&#x4F53;&#x5B66;&#x4E60;&#x5668;&#x7684;&#x6982;&#x5FF5;"></a>&#x4E2A;&#x4F53;&#x5B66;&#x4E60;&#x5668;&#x7684;&#x6982;&#x5FF5;</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#boosting-bagging的概念、异同点"><span class="nav-number">3.</span> <span class="nav-text"><a href="#boosting-bagging&#x7684;&#x6982;&#x5FF5;&#x3001;&#x5F02;&#x540C;&#x70B9;" class="headerlink" title="boosting bagging&#x7684;&#x6982;&#x5FF5;&#x3001;&#x5F02;&#x540C;&#x70B9;"></a>boosting bagging&#x7684;&#x6982;&#x5FF5;&#x3001;&#x5F02;&#x540C;&#x70B9;</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#boosting"><span class="nav-number">3.1.</span> <span class="nav-text"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bagging"><span class="nav-number">3.2.</span> <span class="nav-text"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对比"><span class="nav-number">3.3.</span> <span class="nav-text"><a href="#&#x5BF9;&#x6BD4;" class="headerlink" title="&#x5BF9;&#x6BD4;"></a>&#x5BF9;&#x6BD4;</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#结合策略"><span class="nav-number">4.</span> <span class="nav-text"><a href="#&#x7ED3;&#x5408;&#x7B56;&#x7565;" class="headerlink" title="&#x7ED3;&#x5408;&#x7B56;&#x7565;"></a>&#x7ED3;&#x5408;&#x7B56;&#x7565;</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#平均法"><span class="nav-number">4.1.</span> <span class="nav-text"><a href="#&#x5E73;&#x5747;&#x6CD5;" class="headerlink" title="&#x5E73;&#x5747;&#x6CD5;"></a>&#x5E73;&#x5747;&#x6CD5;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#投票法"><span class="nav-number">4.2.</span> <span class="nav-text"><a href="#&#x6295;&#x7968;&#x6CD5;" class="headerlink" title="&#x6295;&#x7968;&#x6CD5;"></a>&#x6295;&#x7968;&#x6CD5;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习法"><span class="nav-number">4.3.</span> <span class="nav-text"><a href="#&#x5B66;&#x4E60;&#x6CD5;" class="headerlink" title="&#x5B66;&#x4E60;&#x6CD5;"></a>&#x5B66;&#x4E60;&#x6CD5;</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#随机森林"><span class="nav-number">5.</span> <span class="nav-text"><a href="#&#x968F;&#x673A;&#x68EE;&#x6797;" class="headerlink" title="&#x968F;&#x673A;&#x68EE;&#x6797;"></a>&#x968F;&#x673A;&#x68EE;&#x6797;</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#随机森林的思想"><span class="nav-number">5.1.</span> <span class="nav-text"><a href="#&#x968F;&#x673A;&#x68EE;&#x6797;&#x7684;&#x601D;&#x60F3;" class="headerlink" title="&#x968F;&#x673A;&#x68EE;&#x6797;&#x7684;&#x601D;&#x60F3;"></a>&#x968F;&#x673A;&#x68EE;&#x6797;&#x7684;&#x601D;&#x60F3;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机森林的推广"><span class="nav-number">5.2.</span> <span class="nav-text"><a href="#&#x968F;&#x673A;&#x68EE;&#x6797;&#x7684;&#x63A8;&#x5E7F;" class="headerlink" title="&#x968F;&#x673A;&#x68EE;&#x6797;&#x7684;&#x63A8;&#x5E7F;"></a>&#x968F;&#x673A;&#x68EE;&#x6797;&#x7684;&#x63A8;&#x5E7F;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机森林的优缺点"><span class="nav-number">5.3.</span> <span class="nav-text"><a href="#&#x968F;&#x673A;&#x68EE;&#x6797;&#x7684;&#x4F18;&#x7F3A;&#x70B9;" class="headerlink" title="&#x968F;&#x673A;&#x68EE;&#x6797;&#x7684;&#x4F18;&#x7F3A;&#x70B9;"></a>&#x968F;&#x673A;&#x68EE;&#x6797;&#x7684;&#x4F18;&#x7F3A;&#x70B9;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#优点"><span class="nav-number">5.3.1.</span> <span class="nav-text"><a href="#&#x4F18;&#x70B9;" class="headerlink" title="&#x4F18;&#x70B9;"></a>&#x4F18;&#x70B9;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#缺点"><span class="nav-number">5.3.2.</span> <span class="nav-text"><a href="#&#x7F3A;&#x70B9;" class="headerlink" title="&#x7F3A;&#x70B9;"></a>&#x7F3A;&#x70B9;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机森林在sklearn中的参数解释"><span class="nav-number">5.4.</span> <span class="nav-text"><a href="#&#x968F;&#x673A;&#x68EE;&#x6797;&#x5728;sklearn&#x4E2D;&#x7684;&#x53C2;&#x6570;&#x89E3;&#x91CA;" class="headerlink" title="&#x968F;&#x673A;&#x68EE;&#x6797;&#x5728;sklearn&#x4E2D;&#x7684;&#x53C2;&#x6570;&#x89E3;&#x91CA;"></a>&#x968F;&#x673A;&#x68EE;&#x6797;&#x5728;sklearn&#x4E2D;&#x7684;&#x53C2;&#x6570;&#x89E3;&#x91CA;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RandomForestClassifier"><span class="nav-number">5.4.1.</span> <span class="nav-text"><a href="#RandomForestClassifier" class="headerlink" title="RandomForestClassifier"></a>RandomForestClassifier</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#参数（parameters）"><span class="nav-number">5.4.1.1.</span> <span class="nav-text"><a href="#&#x53C2;&#x6570;&#xFF08;parameters&#xFF09;" class="headerlink" title="&#x53C2;&#x6570;&#xFF08;parameters&#xFF09;"></a>&#x53C2;&#x6570;&#xFF08;parameters&#xFF09;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#属性（attributes）"><span class="nav-number">5.4.1.2.</span> <span class="nav-text"><a href="#&#x5C5E;&#x6027;&#xFF08;attributes&#xFF09;" class="headerlink" title="&#x5C5E;&#x6027;&#xFF08;attributes&#xFF09;"></a>&#x5C5E;&#x6027;&#xFF08;attributes&#xFF09;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#方法（methods）"><span class="nav-number">5.4.1.3.</span> <span class="nav-text"><a href="#&#x65B9;&#x6CD5;&#xFF08;methods&#xFF09;" class="headerlink" title="&#x65B9;&#x6CD5;&#xFF08;methods&#xFF09;"></a>&#x65B9;&#x6CD5;&#xFF08;methods&#xFF09;</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RandomForestRegressor"><span class="nav-number">5.4.2.</span> <span class="nav-text"><a href="#RandomForestRegressor" class="headerlink" title="RandomForestRegressor"></a>RandomForestRegressor</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#参数（parameters）-1"><span class="nav-number">5.4.2.1.</span> <span class="nav-text"><a href="#&#x53C2;&#x6570;&#xFF08;parameters&#xFF09;-1" class="headerlink" title="&#x53C2;&#x6570;&#xFF08;parameters&#xFF09;"></a>&#x53C2;&#x6570;&#xFF08;parameters&#xFF09;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#属性（attributes）-1"><span class="nav-number">5.4.2.2.</span> <span class="nav-text"><a href="#&#x5C5E;&#x6027;&#xFF08;attributes&#xFF09;-1" class="headerlink" title="&#x5C5E;&#x6027;&#xFF08;attributes&#xFF09;"></a>&#x5C5E;&#x6027;&#xFF08;attributes&#xFF09;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#方法（methods）-1"><span class="nav-number">5.4.2.3.</span> <span class="nav-text"><a href="#&#x65B9;&#x6CD5;&#xFF08;methods&#xFF09;-1" class="headerlink" title="&#x65B9;&#x6CD5;&#xFF08;methods&#xFF09;"></a>&#x65B9;&#x6CD5;&#xFF08;methods&#xFF09;</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机森林的应用场景"><span class="nav-number">5.5.</span> <span class="nav-text"><a href="#&#x968F;&#x673A;&#x68EE;&#x6797;&#x7684;&#x5E94;&#x7528;&#x573A;&#x666F;" class="headerlink" title="&#x968F;&#x673A;&#x68EE;&#x6797;&#x7684;&#x5E94;&#x7528;&#x573A;&#x666F;"></a>&#x968F;&#x673A;&#x68EE;&#x6797;&#x7684;&#x5E94;&#x7528;&#x573A;&#x666F;</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-number">6.</span> <span class="nav-text"><a href="#&#x53C2;&#x8003;" class="headerlink" title="&#x53C2;&#x8003;"></a>&#x53C2;&#x8003;</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        
<div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2020</span>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
  </script>

  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Datacruiser</span>

  
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_pv">
        本站总访问量: <span id="busuanzi_value_site_pv"></span>次
    </span>
    <span id="busuanzi_container_site_uv">
        本站访客数<span id="busuanzi_value_site_uv"></span>人次
    </span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">275.3k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>








        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  

    
      <script id="dsq-count-scr" src="https://datacruiser.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://datacruiser.io/2019/08/05/随机森林算法梳理/';
          this.page.identifier = '2019/08/05/随机森林算法梳理/';
          this.page.title = '随机森林算法梳理';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://datacruiser.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
